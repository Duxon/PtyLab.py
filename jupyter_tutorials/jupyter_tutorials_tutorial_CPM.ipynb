{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief introduction to ptychograhy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ptyLab introduction\n",
    "\n",
    "ptychoLab is a highly modular coding package which can be used for both conventional and Fourier ptychography reconstruction. Due to the modular nature it is easy to modify the code for various tasks and add your own functions, such as a new reconstruction engine or a calibration routine. To understand ptychoLab we need to understand the basic classes contained within, which are briefly outlined below:\n",
    "\n",
    "\n",
    " -  ExperimentalData - this class is used to import the experimental data from an .hdf5 file. If file contains the experimental images stored as an image stack (called ptychogram), probe/LED positions and several experimental parameters then the data can be successfully imported and reconstructed.\n",
    " -  Optimizer - this class creates various objects from the immutable experimentalData class which will be optimized i.e. are mutable. \n",
    " -  Params - this class stores parameters use for the reconstruction such that they could be exported/imported for different experiments\n",
    " -  Engines - all the engines used for the reconstruction are based on this class, which take the experimentalData and Optimizer objects as parameters and perform object/probe reconstruction.\n",
    " -  Monitor - visualization class used to display the reconstruction process\n",
    " -  CalibrationFPM - this package contains k-space position calibration routines caused by misaligned LED positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %gui qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import fracPy\n",
    "from fracPy.io import getExampleDataFolder\n",
    "from fracPy import Engines\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('tkagg')\n",
    "\n",
    "font = {'family' : 'sans-serif','size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "colormap = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ePIE:Sucesfully created ePIE ePIE_engine\n",
      "INFO:ePIE:Wavelength attribute: 4.4999998749517545e-07\n"
     ]
    }
   ],
   "source": [
    "fileName = 'Lenspaper.hdf5'  \n",
    "filePath = getExampleDataFolder() / fileName\n",
    "\n",
    "optimizable, exampleData, params, monitor, ePIE_engine = fracPy.easy_initialize(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperimentalData class\n",
    "The data must be stored as a \".hdf5\" file, which convieniently enables structured file storage. For ptychographic data, experimental parameters such as wavelenght or pixel size can be convieniently stored together with illumination positions and the actual raw images in a single file.\n",
    "\n",
    "Full list of fields required for ptyLab to work are:\n",
    "- ptychogram - 3D image stack \n",
    "- wavelength - illumination lambda\n",
    "- encoder - diffracted field positions\n",
    "- dxd - pixel size\n",
    "- zo - sample to detector distance\n",
    "\n",
    "Also we have optional fields because they will either be computed later from the \"required_fields\" or are required for FPM, but not CPM (or vice-versa). If not provided by the user they will be set as None.\n",
    "- magnification - magnification, used for FPM computations of dxp\n",
    "- dxp - dxp, can be provided by the user, otherwise will be computed using dxp=dxd/magnification\n",
    "- No -  number of upsampled pixels\n",
    "- Nd -  probe/pupil plane size, will be set to Ptychogram size by default\n",
    "- entrancePupilDiameter -  entrance pupil diameter, defined in lens-based microscopes as the aperture diameter, reqquired for FPM\n",
    "- spectralDensity\n",
    "\n",
    "Apart from 1D experimental parameter values (NOTE: defined in meters!) there are other two important fields.\n",
    "\n",
    "The \".hdf5\" file must contain a field called \"ptychogram\" containing the experimental raw images as a 3D array of shape [N,X,Y], where N is the number of images corresponding to each illumination vector in the \"encoder\" and X-Y are the 2D image dimensions. \n",
    "\n",
    "The \".hdf5\" file must have a field called \"encoder\" containing the positions of the illumination in meters and has a 2D shape [N,2], where N is the number of illumination positions used on a X-Y grid.\n",
    "\n",
    "We start off our demonstration by creating the ExperimentalData() class which is used to load the .hdf5 file. In this example the varible \"exampleData\" will contain our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum count in ptychogram is 14201\n"
     ]
    }
   ],
   "source": [
    "# now, all our experimental data is loaded into experimental_data and we don't have to worry about it anymore.\n",
    "exampleData.entrancePupilDiameter = optimizable.Np / 3 * optimizable.dxp  # initial estimate of beam\n",
    "exampleData.showPtychogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizable class\n",
    "The ExperimentalData class contains immutable values. The Optimizable class creates object which will be mutable during the reconstruction or calibration procedure. These objects will be the reconstructed object and the probe/pupil. We can use switches to determine how the data should be prepared for the reconstruction. Once everything is set use \"prepare_reconstruction()\" method.\n",
    "\n",
    "In FPM the intial object estimate can be computed from the raw data. \"Optimizable.initialObject = 'upsampled'\" will take the low-resolution raw data and create an upsampled object estimate via interpolation. The probe/pupil is set to be a clear circle representing a fully transparent aperture without any aberration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Optimizables:Initial object set to ones\n",
      "INFO:Optimizables:Initial probe set to circ\n"
     ]
    }
   ],
   "source": [
    "# now create an object to hold everything we're eventually interested in\n",
    "optimizable.npsm = 1 # Number of probe modes to reconstruct\n",
    "optimizable.nosm = 1 # Number of object modes to reconstruct\n",
    "optimizable.nlambda = 1 # len(exampleData.spectralDensity) # Number of wavelength\n",
    "optimizable.nslice = 1 # Number of object slice\n",
    "\n",
    "# set initial guesses\n",
    "optimizable.initialProbe = 'circ'\n",
    "optimizable.initialObject = 'ones'\n",
    "# initialize probe and object and related params\n",
    "optimizable.prepare_reconstruction()\n",
    "\n",
    "# customize initial probe quadratic phase\n",
    "optimizable.probe = optimizable.probe*np.exp(1.j*2*np.pi/optimizable.wavelength * \n",
    "                                             (optimizable.Xp**2+optimizable.Yp**2)/(2*6e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor class\n",
    "This class will create a monitor to visualize the reconstruction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set monitor properties\n",
    "monitor.figureUpdateFrequency = 1\n",
    "monitor.objectPlot = 'complex'  # complex abs angle\n",
    "monitor.verboseLevel = 'high'  # high: plot two figures, low: plot only one figure\n",
    "monitor.objectPlotZoom = 1.5   # control object plot FoV\n",
    "monitor.probePlotZoom = 0.5   # control probe plot FoV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param class\n",
    "This class holds the parameters to be used on the optimizable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main parameters\n",
    "params.positionOrder = 'random'  # 'sequential' or 'random'\n",
    "params.propagator = 'Fresnel'  # Fraunhofer Fresnel ASP scaledASP polychromeASP scaledPolychromeASP\n",
    "\n",
    "## how do we want to reconstruct?\n",
    "params.gpuSwitch = True\n",
    "params.probePowerCorrectionSwitch = True\n",
    "params.modulusEnforcedProbeSwitch = False\n",
    "params.comStabilizationSwitch = True\n",
    "params.orthogonalizationSwitch = False\n",
    "params.orthogonalizationFrequency = 10\n",
    "params.fftshiftSwitch = False\n",
    "params.intensityConstraint = 'standard'  # standard fluctuation exponential poission\n",
    "params.absorbingProbeBoundary = False\n",
    "params.objectContrastSwitch = False\n",
    "params.absObjectSwitch = False\n",
    "params.backgroundModeSwitch = False\n",
    "params.couplingSwitch = True\n",
    "params.couplingAleph = 1\n",
    "params.positionCorrectionSwitch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mqNewton:Sucesfully created momentum accelerated qNewton engine\n",
      "INFO:mqNewton:Wavelength attribute: 4.4999998749517545e-07\n",
      "d:\\du\\workshop\\fracpy\\fracPy\\Engines\\BaseReconstructor.py:254: UserWarning: fftshiftSwitch set to false, this may lead to reduced performance\n",
      "  warnings.warn('fftshiftSwitch set to false, this may lead to reduced performance')\n",
      "INFO:mqNewton:switch to gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum Engines implemented: momentum, ADAM, NADAM\n",
      "Momentum engine used: NADAM\n",
      "                                                                                                                       \n",
      "iteration: 0                                                                                                           \n",
      "error: 245.9                                                                                                           \n",
      "estimated linear overlap: 31.6 %                                                                                       \n",
      "estimated area overlap: 66.4 %                                                                                         \n",
      "                                                                                                                       \n",
      "iteration: 1                                                                                                           \n",
      "error: 161.1                                                                                                           \n",
      "estimated linear overlap: 25.2 %                                                                                       \n",
      "estimated area overlap: 67.6 %                                                                                         \n",
      "                                                                                                                       \n",
      "iteration: 2                                                                                                           \n",
      "error: 116.5                                                                                                           \n",
      "estimated linear overlap: 27.2 %                                                                                       \n",
      "estimated area overlap: 70.0 %                                                                                         \n",
      "                                                                                                                       \n",
      "iteration: 3                                                                                                           \n",
      "error: 107.7                                                                                                           \n",
      "estimated linear overlap: 24.9 %                                                                                       \n",
      "estimated area overlap: 68.4 %                                                                                         \n",
      "                                                                                                                       \n",
      "iteration: 4                                                                                                           \n",
      "error: 94.2                                                                                                            \n",
      "estimated linear overlap: 24.5 %                                                                                       \n",
      "estimated area overlap: 68.1 %                                                                                         \n",
      "mqNewton: 100%|██████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mqNewton:switch to cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the reconstruction\n",
    "# select reconstrution engine\n",
    "engine_mqNewton = Engines.mqNewton(optimizable, exampleData, params, monitor)\n",
    "engine_mqNewton.numIterations = 5\n",
    "engine_mqNewton.betaProbe = 1\n",
    "engine_mqNewton.betaObject = 1\n",
    "engine_mqNewton.beta1 = 0.5\n",
    "engine_mqNewton.beta2 = 0.5\n",
    "engine_mqNewton.betaProbe_m = 1\n",
    "engine_mqNewton.betaObject_m = 1\n",
    "engine_mqNewton.momentum_method = 'NADAM'\n",
    "engine_mqNewton.doReconstruction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine class\n",
    "A specific engine can be imported (e.g. ePIE, mPIE etc.) and used on the optimizable class to \"optimize\" our initial estimates for the object/probe. In this example we use the quasi-Newton method to reconstruct the data, by passing the optimizable, exampleData and monitor objects to be used during the reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ePIE:Sucesfully created ePIE ePIE_engine\n",
      "INFO:ePIE:Wavelength attribute: 4.4999998749517545e-07\n",
      "INFO:ePIE:switch to gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \n",
      "iteration: 0                                                                                                           \n",
      "error: 245.9                                                                                                           \n",
      "estimated linear overlap: 24.3 %                                                                                       \n",
      "estimated area overlap: 69.7 %                                                                                         \n",
      "                                                                                                                       \n",
      "iteration: 1                                                                                                           \n",
      "error: 161.1                                                                                                           \n",
      "estimated linear overlap: 24.0 %                                                                                       \n",
      "estimated area overlap: 69.4 %                                                                                         \n",
      "                                                                                                                       \n",
      "iteration: 2                                                                                                           \n",
      "error: 116.5                                                                                                           \n",
      "estimated linear overlap: 23.5 %                                                                                       \n",
      "estimated area overlap: 68.9 %                                                                                         \n",
      "                                                                                                                       \n",
      "iteration: 3                                                                                                           \n",
      "error: 107.7                                                                                                           \n",
      "estimated linear overlap: 23.3 %                                                                                       \n",
      "estimated area overlap: 69.2 %                                                                                         \n",
      "ePIE:  80%|██████████████████████████████████████████████████████████████▍               | 4/5 [00:02<00:00,  1.61it/s]"
     ]
    }
   ],
   "source": [
    "## switch engine if you need\n",
    "# ePIE\n",
    "engine_ePIE = Engines.ePIE(optimizable, exampleData, params,monitor)\n",
    "engine_ePIE.numIterations = 5\n",
    "engine_ePIE.betaProbe = 0.25\n",
    "engine_ePIE.betaObject = 0.25\n",
    "engine_ePIE.doReconstruction()\n",
    "\n",
    "# mPIE\n",
    "engine_mPIE = Engines.mPIE(optimizable, exampleData, params, monitor)\n",
    "engine_mPIE.numIterations = 5\n",
    "engine_mPIE.betaProbe = 0.25\n",
    "engine_mPIE.betaObject = 0.25\n",
    "engine_mPIE.doReconstruction()\n",
    "\n",
    "# zPIE\n",
    "engine_zPIE = Engines.zPIE(optimizable, exampleData, params, monitor)\n",
    "engine_zPIE.numIterations = 5\n",
    "engine_zPIE.betaProbe = 0.35\n",
    "engine_zPIE.betaObject = 0.35\n",
    "engine_zPIE.zPIEgradientStepSize = 100  # gradient step size for axial position correction (typical range [1, 100])\n",
    "engine_zPIE.doReconstruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the data\n",
    "# optimizable.saveResults('reconstruction.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
